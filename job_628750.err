+ module load toolkit/nvidia-hpc-sdk/25.1
++ /opt/bwhpc/common/admin/modules/module-wrapper/modulecmd bash load toolkit/nvidia-hpc-sdk/25.1
+ eval 'CC=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/bin/nvc;' export 'CC;' '__LMOD_REF_COUNT_CPATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/include/qd:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nvshmem/include:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nccl/include:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/math_libs/include:1;' export '__LMOD_REF_COUNT_CPATH;' 'CPATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/include/qd:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nvshmem/include:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nccl/include:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/math_libs/include;' export 'CPATH;' '__LMOD_REF_COUNT_CPLUS_INCLUDE_PATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/include/qd:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nvshmem/include:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nccl/include:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/math_libs/include:1;' export '__LMOD_REF_COUNT_CPLUS_INCLUDE_PATH;' 'CPLUS_INCLUDE_PATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/include/qd:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nvshmem/include:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nccl/include:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/math_libs/include;' export 'CPLUS_INCLUDE_PATH;' 'CPP=cpp;' export 'CPP;' 'CXX=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/bin/nvc++;' export 'CXX;' '__LMOD_REF_COUNT_C_INCLUDE_PATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/include/qd:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nvshmem/include:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nccl/include:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/math_libs/include:1;' export '__LMOD_REF_COUNT_C_INCLUDE_PATH;' 'C_INCLUDE_PATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/include/qd:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nvshmem/include:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nccl/include:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/math_libs/include;' export 'C_INCLUDE_PATH;' 'F77=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/bin/nvfortran;' export 'F77;' 'F90=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/bin/nvfortran;' export 'F90;' 'FC=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/bin/nvfortran;' export 'FC;' 'KIT_FAMILY_COMPILER=toolkit/nvidia-hpc-sdk;' export 'KIT_FAMILY_COMPILER;' 'KIT_FAMILY_COMPILER_VERSION=25.1;' export 'KIT_FAMILY_COMPILER_VERSION;' 'KIT_FAMILY_MPI=toolkit/nvidia-hpc-sdk;' export 'KIT_FAMILY_MPI;' 'KIT_FAMILY_MPI_VERSION=25.1;' export 'KIT_FAMILY_MPI_VERSION;' '__LMOD_REF_COUNT_LD_LIBRARY_PATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nvshmem/lib:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nccl/lib:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/math_libs/lib64:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/lib:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/lib:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/cuda/lib64:1;' export '__LMOD_REF_COUNT_LD_LIBRARY_PATH;' 'LD_LIBRARY_PATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nvshmem/lib:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nccl/lib:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/math_libs/lib64:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/lib:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/lib:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/cuda/lib64;' export 'LD_LIBRARY_PATH;' 'LMOD_FAMILY_COMPILER=toolkit/nvidia-hpc-sdk;' export 'LMOD_FAMILY_COMPILER;' 'LMOD_FAMILY_COMPILER_VERSION=25.1;' export 'LMOD_FAMILY_COMPILER_VERSION;' 'LMOD_FAMILY_MPI=toolkit/nvidia-hpc-sdk;' export 'LMOD_FAMILY_MPI;' 'LMOD_FAMILY_MPI_VERSION=25.1;' export 'LMOD_FAMILY_MPI_VERSION;' 'LOADEDMODULES=toolkit/nvidia-hpc-sdk/25.1;' export 'LOADEDMODULES;' '__LMOD_REF_COUNT_MANPATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/man:1\;/usr/share/lmod/lmod/share/man:1\;/usr/share/man:1\;:1;' export '__LMOD_REF_COUNT_MANPATH;' 'MANPATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/man:/usr/share/lmod/lmod/share/man:/usr/share/man::;' export 'MANPATH;' '__LMOD_REF_COUNT_MODULEPATH=/opt/bwhpc/fr/modulefiles:1\;/opt/bwhpc/common/modulefiles:1;' export '__LMOD_REF_COUNT_MODULEPATH;' 'MODULEPATH=/opt/bwhpc/fr/modulefiles:/opt/bwhpc/common/modulefiles;' export 'MODULEPATH;' 'NVHPC=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1;' export 'NVHPC;' 'NVHPC_ROOT=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1;' export 'NVHPC_ROOT;' '__LMOD_REF_COUNT_PATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/bin:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/mpi/bin:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/bin:1\;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/cuda/bin:1\;/home/fr/fr_fr/fr_fl184/.local/bin:1\;/home/fr/fr_fr/fr_fl184/bin:1\;/software/all/bin:1\;/usr/share/Modules/bin:1\;/usr/local/bin:1\;/usr/bin:1\;/usr/local/sbin:1\;/usr/sbin:1;' export '__LMOD_REF_COUNT_PATH;' 'PATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/bin:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/mpi/bin:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/bin:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/cuda/bin:/home/fr/fr_fr/fr_fl184/.local/bin:/home/fr/fr_fr/fr_fl184/bin:/software/all/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin;' export 'PATH;' '_LMFILES_=/opt/bwhpc/common/modulefiles/toolkit/nvidia-hpc-sdk/25.1.lua;' export '_LMFILES_;' '_ModuleTable001_=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpjb21waWxlciA9ICJ0b29sa2l0L252aWRpYS1ocGMtc2RrIiwKbXBpID0gInRvb2xraXQvbnZpZGlhLWhwYy1zZGsiLAp9LAptVCA9IHsKWyJ0b29sa2l0L252aWRpYS1ocGMtc2RrIl0gPSB7CmZuID0gIi9vcHQvYndocGMvY29tbW9uL21vZHVsZWZpbGVzL3Rvb2xraXQvbnZpZGlhLWhwYy1zZGsvMjUuMS5sdWEiLApmdWxsTmFtZSA9ICJ0b29sa2l0L252aWRpYS1ocGMtc2RrLzI1LjEiLApsb2FkT3JkZXIgPSAxLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIs;' export '_ModuleTable001_;' '_ModuleTable002_=CnVzZXJOYW1lID0gInRvb2xraXQvbnZpZGlhLWhwYy1zZGsvMjUuMSIsCndWID0gIjAwMDAwMDAyNS4wMDAwMDAwMDEuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKIi9vcHQvYndocGMvZnIvbW9kdWxlZmlsZXMiLCAiL29wdC9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL29wdC9id2hwYy9mci9tb2R1bGVmaWxlczovb3B0L2J3aHBjL2NvbW1vbi9tb2R1bGVmaWxlcyIsCn0K;' export '_ModuleTable002_;' '_ModuleTable_Sz_=2;' export '_ModuleTable_Sz_;'
++ CC=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/bin/nvc
++ export CC
++ __LMOD_REF_COUNT_CPATH='/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/include/qd:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nvshmem/include:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nccl/include:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/math_libs/include:1'
++ export __LMOD_REF_COUNT_CPATH
++ CPATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/include/qd:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nvshmem/include:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nccl/include:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/math_libs/include
++ export CPATH
++ __LMOD_REF_COUNT_CPLUS_INCLUDE_PATH='/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/include/qd:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nvshmem/include:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nccl/include:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/math_libs/include:1'
++ export __LMOD_REF_COUNT_CPLUS_INCLUDE_PATH
++ CPLUS_INCLUDE_PATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/include/qd:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nvshmem/include:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nccl/include:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/math_libs/include
++ export CPLUS_INCLUDE_PATH
++ CPP=cpp
++ export CPP
++ CXX=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/bin/nvc++
++ export CXX
++ __LMOD_REF_COUNT_C_INCLUDE_PATH='/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/include/qd:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nvshmem/include:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nccl/include:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/math_libs/include:1'
++ export __LMOD_REF_COUNT_C_INCLUDE_PATH
++ C_INCLUDE_PATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/include/qd:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nvshmem/include:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nccl/include:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/math_libs/include
++ export C_INCLUDE_PATH
++ F77=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/bin/nvfortran
++ export F77
++ F90=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/bin/nvfortran
++ export F90
++ FC=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/bin/nvfortran
++ export FC
++ KIT_FAMILY_COMPILER=toolkit/nvidia-hpc-sdk
++ export KIT_FAMILY_COMPILER
++ KIT_FAMILY_COMPILER_VERSION=25.1
++ export KIT_FAMILY_COMPILER_VERSION
++ KIT_FAMILY_MPI=toolkit/nvidia-hpc-sdk
++ export KIT_FAMILY_MPI
++ KIT_FAMILY_MPI_VERSION=25.1
++ export KIT_FAMILY_MPI_VERSION
++ __LMOD_REF_COUNT_LD_LIBRARY_PATH='/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nvshmem/lib:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nccl/lib:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/math_libs/lib64:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/lib:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/lib:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/cuda/lib64:1'
++ export __LMOD_REF_COUNT_LD_LIBRARY_PATH
++ LD_LIBRARY_PATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nvshmem/lib:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/nccl/lib:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/math_libs/lib64:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/lib:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/lib:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/cuda/lib64
++ export LD_LIBRARY_PATH
++ LMOD_FAMILY_COMPILER=toolkit/nvidia-hpc-sdk
++ export LMOD_FAMILY_COMPILER
++ LMOD_FAMILY_COMPILER_VERSION=25.1
++ export LMOD_FAMILY_COMPILER_VERSION
++ LMOD_FAMILY_MPI=toolkit/nvidia-hpc-sdk
++ export LMOD_FAMILY_MPI
++ LMOD_FAMILY_MPI_VERSION=25.1
++ export LMOD_FAMILY_MPI_VERSION
++ LOADEDMODULES=toolkit/nvidia-hpc-sdk/25.1
++ export LOADEDMODULES
++ __LMOD_REF_COUNT_MANPATH='/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/man:1;/usr/share/lmod/lmod/share/man:1;/usr/share/man:1;:1'
++ export __LMOD_REF_COUNT_MANPATH
++ MANPATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/man:/usr/share/lmod/lmod/share/man:/usr/share/man::
++ export MANPATH
++ __LMOD_REF_COUNT_MODULEPATH='/opt/bwhpc/fr/modulefiles:1;/opt/bwhpc/common/modulefiles:1'
++ export __LMOD_REF_COUNT_MODULEPATH
++ MODULEPATH=/opt/bwhpc/fr/modulefiles:/opt/bwhpc/common/modulefiles
++ export MODULEPATH
++ NVHPC=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1
++ export NVHPC
++ NVHPC_ROOT=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1
++ export NVHPC_ROOT
++ __LMOD_REF_COUNT_PATH='/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/bin:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/mpi/bin:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/bin:1;/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/cuda/bin:1;/home/fr/fr_fr/fr_fl184/.local/bin:1;/home/fr/fr_fr/fr_fl184/bin:1;/software/all/bin:1;/usr/share/Modules/bin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1'
++ export __LMOD_REF_COUNT_PATH
++ PATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/extras/qd/bin:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/comm_libs/mpi/bin:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/compilers/bin:/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/cuda/bin:/home/fr/fr_fr/fr_fl184/.local/bin:/home/fr/fr_fr/fr_fl184/bin:/software/all/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export PATH
++ _LMFILES_=/opt/bwhpc/common/modulefiles/toolkit/nvidia-hpc-sdk/25.1.lua
++ export _LMFILES_
++ _ModuleTable001_=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpjb21waWxlciA9ICJ0b29sa2l0L252aWRpYS1ocGMtc2RrIiwKbXBpID0gInRvb2xraXQvbnZpZGlhLWhwYy1zZGsiLAp9LAptVCA9IHsKWyJ0b29sa2l0L252aWRpYS1ocGMtc2RrIl0gPSB7CmZuID0gIi9vcHQvYndocGMvY29tbW9uL21vZHVsZWZpbGVzL3Rvb2xraXQvbnZpZGlhLWhwYy1zZGsvMjUuMS5sdWEiLApmdWxsTmFtZSA9ICJ0b29sa2l0L252aWRpYS1ocGMtc2RrLzI1LjEiLApsb2FkT3JkZXIgPSAxLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIs
++ export _ModuleTable001_
++ _ModuleTable002_=CnVzZXJOYW1lID0gInRvb2xraXQvbnZpZGlhLWhwYy1zZGsvMjUuMSIsCndWID0gIjAwMDAwMDAyNS4wMDAwMDAwMDEuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKIi9vcHQvYndocGMvZnIvbW9kdWxlZmlsZXMiLCAiL29wdC9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL29wdC9id2hwYy9mci9tb2R1bGVmaWxlczovb3B0L2J3aHBjL2NvbW1vbi9tb2R1bGVmaWxlcyIsCn0K
++ export _ModuleTable002_
++ _ModuleTable_Sz_=2
++ export _ModuleTable_Sz_
+ export CC=gcc
+ CC=gcc
+ export CXX=g++
+ CXX=g++
+ export CUDA_PATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/cuda
+ CUDA_PATH=/software/bwhpc/common/toolkit/nvidia_hpc_sdk/25.1/Linux_x86_64/25.1/cuda
+ cd /home/fr/fr_fr/fr_fl184/MightyBatticeLoltzmann
+ meson setup buildDir --wipe --buildtype=release
+ ninja -C buildDir implementations/cuda_mpi/cuda_mpi_04
+ mpirun --bind-to none --map-by ppr:4:node -np 8 ./buildDir/implementations/cuda_mpi/cuda_mpi_04 input_04.txt

+ mpirun --bind-to none --map-by ppr:8:node -np 16 ./buildDir/implementations/cuda_mpi/cuda_mpi_04 input_04.txt
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 8
slots that were requested by the application:

  ./buildDir/implementations/cuda_mpi/cuda_mpi_04

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
+ mpirun --bind-to none --map-by ppr:12:node -np 24 ./buildDir/implementations/cuda_mpi/cuda_mpi_04 input_04.txt
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 12
slots that were requested by the application:

  ./buildDir/implementations/cuda_mpi/cuda_mpi_04

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
+ mpirun --bind-to none --map-by ppr:16:node -np 32 ./buildDir/implementations/cuda_mpi/cuda_mpi_04 input_04.txt
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 16
slots that were requested by the application:

  ./buildDir/implementations/cuda_mpi/cuda_mpi_04

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
+ mpirun --bind-to none --map-by ppr:20:node -np 40 ./buildDir/implementations/cuda_mpi/cuda_mpi_04 input_04.txt
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 20
slots that were requested by the application:

  ./buildDir/implementations/cuda_mpi/cuda_mpi_04

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
+ mpirun --bind-to none --map-by ppr:24:node -np 48 ./buildDir/implementations/cuda_mpi/cuda_mpi_04 input_04.txt
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 24
slots that were requested by the application:

  ./buildDir/implementations/cuda_mpi/cuda_mpi_04

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
+ mpirun --bind-to none --map-by ppr:28:node -np 56 ./buildDir/implementations/cuda_mpi/cuda_mpi_04 input_04.txt
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 28
slots that were requested by the application:

  ./buildDir/implementations/cuda_mpi/cuda_mpi_04

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
+ mpirun --bind-to none --map-by ppr:32:node -np 64 ./buildDir/implementations/cuda_mpi/cuda_mpi_04 input_04.txt
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 32
slots that were requested by the application:

  ./buildDir/implementations/cuda_mpi/cuda_mpi_04

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
+ mpirun --bind-to none --map-by ppr:36:node -np 72 ./buildDir/implementations/cuda_mpi/cuda_mpi_04 input_04.txt
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 36
slots that were requested by the application:

  ./buildDir/implementations/cuda_mpi/cuda_mpi_04

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
+ mpirun --bind-to none --map-by ppr:40:node -np 80 ./buildDir/implementations/cuda_mpi/cuda_mpi_04 input_04.txt
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 40
slots that were requested by the application:

  ./buildDir/implementations/cuda_mpi/cuda_mpi_04

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
